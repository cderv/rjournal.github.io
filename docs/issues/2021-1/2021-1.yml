issue: 2021-1
year: 2021
volume: 13
num: 1
month: June
bibmonth: june
articles:
- slug: editorial
  cat: Editorial
  author: Di Cook
  title: Editorial
  bibtitle: Editorial
  bibauthor: Di Cook
  pages:
  - '4'
  - '6'
- heading: Contributed Research Articles
- slug: RJ-2021-026
  title: 'SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and
    Partial Least Squares'
  bibtitle: |-
    SEEDCCA: An Integrated R-Package for Canonical Correlation
              Analysis and Partial Least Squares
  author:
  - Bo-Young Kim
  - Yunju Im
  - Jae Keun Yoo
  bibauthor: Bo-Young Kim and Yunju Im and Jae Keun Yoo
  abstract: '  Abstract Canonical correlation analysis (CCA) has a long history as
    an explanatory statistical method            in high-dimensional data analysis
    and has been successfully applied in many scientific fields such as            chemometrics,
    pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA
    is a            newly developed R package that implements not only the standard
    and seeded CCA but also partial            least squares. The package enables
    us to fit CCA to large-p and small-n data. The paper provides a            complete
    guide. Also, the seeded CCA application results are compared with the regularized
    CCA in            the existing R package. It is believed that the package, along
    with the paper, will contribute to high           dimensional data analysis in
    various science field practitioners and that the statistical methodologies            in
    multivariate analysis become more fruitful.'
  acknowledged: '2019-07-18'
  online: '2021-06-07'
  landing: '2021'
  pages:
  - '7'
  - '20'
- slug: RJ-2021-027
  title: 'npcure: An R Package for Nonparametric Inference in Mixture Cure Models'
  bibtitle: |-
    npcure: An R Package for Nonparametric Inference in Mixture
              Cure Models
  author:
  - Ana López-Cheda
  - M. Amalia Jácome
  - Ignacio López-de-Ullibarri
  bibauthor: |-
    Ana López-Cheda and M. Amalia Jácome and Ignacio López-de-
              Ullibarri
  abstract: '  Abstract Mixture cure models have been widely used to analyze survival
    data with a cure fraction.            They assume that a subgroup of the individuals
    under study will never experience the event (cured            subjects). So, the
    goal is twofold: to study both the cure probability and the failure time of the            uncured
    individuals through a proper survival function (latency). The R package npcure
    implements a            completely nonparametric approach for estimating these
    functions in mixture cure models, considering            right-censored survival
    times. Nonparametric estimators for the cure probability and the latency as            functions
    of a covariate are provided. Bootstrap bandwidth selectors for the estimators
    are included.            The package also implements a nonparametric covariate
    significance test for the cure probability,            which can be applied with
    a continuous, discrete, or qualitative covariate.'
  acknowledged: '2019-07-19'
  online: '2021-06-07'
  CRANpkgs:
  - smcure
  - NPHMC
  - flexsurvcure
  - rcure
  - geecure
  - miCoPTCM
  - mixcure
  - GORcure
  - intercure
  - npcure
  - condSURV
  - prodlim
  - survidm
  - KMsurv
  CTV_rev: Survival
  suppl: unknown
  landing: '2021'
  pages:
  - '21'
  - '41'
- slug: RJ-2021-056
  title: A Method for Deriving Information from Running R Code
  bibtitle: A Method for Deriving Information from Running R Code
  author: Mark P.J. van der Loo
  bibauthor: Mark P.J. van der Loo
  abstract: '  Abstract It is often useful to tap information from a running R script.
    Obvious use cases include            monitoring the consumption of resources (time,
    memory) and logging. Perhaps less obvious cases            include tracking changes
    in R objects or collecting the output of unit tests. In this paper, we demonstrate            an
    approach that abstracts the collection and processing of such secondary information
    from the            running R script. Our approach is based on a combination of
    three elements. The first element is            to build a customized way to evaluate
    code. The second is labeled local masking and it involves            temporarily
    masking a user-facing function so an alternative version of it is called. The
    third element            we label local side effect. This refers to the fact that
    the masking function exports information to the            secondary information
    flow without altering a global state. The result is a method for building systems            in
    pure R that lets users create and control secondary flows of information with
    minimal impact on            their workflow and no global side effects.'
  acknowledged: '2019-10-20'
  online: '2021-07-13'
  suppl: unknown
  landing: '2021'
  pages:
  - '42'
  - '52'
- slug: RJ-2021-028
  title: 'JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival
    Data with Competing Risks'
  bibtitle: |-
    JMcmprsk: An R Package for Joint Modelling of Longitudinal
              and Survival Data with Competing Risks
  author:
  - Hong Wang
  - Ning Li
  - Shanpeng Li
  - Gang Li
  bibauthor: Hong Wang and Ning Li and Shanpeng Li and Gang Li
  abstract: '  Abstract In this paper, we describe an R package named JMcmprsk, for
    joint modelling of longitudinal            and survival data with competing risks.
    The package in its current version implements two joint            models of longitudinal
    and survival data proposed to handle competing risks survival data together            with
    continuous and ordinal longitudinal outcomes respectively (Elashoff et al., 2008;
    Li et al., 2010).            The corresponding R implementations are further illustrated
    with real examples. The package also            provides simulation functions
    to simulate datasets for joint modelling with continuous or ordinal            outcomes
    under the competing risks scenario, which provide useful tools to validate and
    evaluate            new joint modelling methods.'
  acknowledged: '2020-04-04'
  online: '2021-06-07'
  suppl: unknown
  landing: '2021'
  pages:
  - '53'
  - '68'
- slug: RJ-2021-029
  title: Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package
  bibtitle: |-
    Wide-to-tall Data Reshaping Using Regular Expressions and
              the nc Package
  author: Toby Dylan Hocking
  bibauthor: Toby Dylan Hocking
  abstract: '  Abstract Regular expressions are powerful tools for extracting tables
    from non-tabular text data.            Capturing regular expressions that describe
    the information to extract from column names can be            especially useful
    when reshaping a data table from wide (few rows with many regularly named            columns)
    to tall (fewer columns with more rows). We present the R package nc (short for
    named            capture), which provides functions for wide-to-tall data reshaping
    using regular expressions. We            describe the main new ideas of nc, and
    provide detailed comparisons with related R packages (stats,            utils,
    data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).'
  acknowledged: '2020-04-30'
  online: '2021-06-07'
  CRANpkgs:
  - ggplot2
  - nc
  - namedCapture
  - rematch2
  - rex
  - stringr
  - stringi
  - tidyr
  - re2r
  - reshape2
  - tidyfast
  - tidyfst
  - cdata
  - microbenchmark
  CTV_rev:
  - Graphics
  - NaturalLanguageProcessing
  - Phylogenetics
  - TeachingStatistics
  suppl: unknown
  landing: '2021'
  pages:
  - '69'
  - '82'
- slug: RJ-2021-030
  title: 'Linear Regression with Stationary Errors: the R Package slm'
  bibtitle: 'Linear Regression with Stationary Errors: the R Package slm'
  author:
  - Emmanuel Caron
  - Jérôme Dedecker
  - Bertrand Michel
  bibauthor: Emmanuel Caron and Jérôme Dedecker and Bertrand Michel
  abstract: ' Abstract This paper introduces the R package slm, which stands for Stationary
    Linear Models.           The package contains a set of statistical procedures
    for linear regression in the general context where           the error process
    is strictly stationary with a short memory. We work in the setting of Hannan (1973),           who
    proved the asymptotic normality of the (normalized) least squares estimators (LSE)
    under           very mild conditions on the error process. We propose different
    ways to estimate the asymptotic           covariance matrix of the LSE and then
    to correct the type I error rates of the usual tests on the           parameters
    (as well as confidence intervals). The procedures are evaluated through different
    sets of           simulations.'
  acknowledged: '2020-05-01'
  online: '2021-06-07'
  CRANpkgs:
  - slm
  - sandwich
  - stats
  - capushe
  CTV_rev:
  - Econometrics
  - Finance
  - Robust
  - SocialSciences
  suppl: unknown
  landing: '2021'
  pages:
  - '83'
  - '100'
- slug: RJ-2021-031
  title: 'exPrior: An R Package for the Formulation of Ex-Situ Priors'
  bibtitle: 'exPrior: An R Package for the Formulation of Ex-Situ Priors'
  author:
  - Falk Heße
  - Karina Cucchi
  - Nura Kawa
  - Yoram Rubin
  bibauthor: Falk Heße and Karina Cucchi and Nura Kawa and Yoram Rubin
  abstract: '  Abstract The exPrior package implements a procedure for formulating
    informative priors of geo           statistical properties for a target field
    site, called ex-situ priors and introduced in Cucchi et al. (2019).            The
    procedure uses a Bayesian hierarchical model to assimilate multiple types of data
    coming from            multiple sites considered as similar to the target site.
    This prior summarizes the information contained            in the data in the
    form of a probability density function that can be used to better inform further            geostatistical
    investigations at the site. The formulation of the prior uses ex-situ data, where
    the data            set can either be gathered by the user or come in the form
    of a structured database. The package is            designed to be flexible in
    that regard. For illustration purposes and for easiness of use, the package is            ready
    to be used with the worldwide hydrogeological parameter database (WWHYPDA) Comunian            and
    Renard (2009).'
  acknowledged: '2020-04-04'
  online: '2021-06-07'
  CRANpkgs:
  - exPrior
  - geoR
  - gstat
  - georob
  - aqp
  - spBayes
  - spTimer
  - BayesNSGP
  - anchoredDistr
  - nimble
  - RSQLite
  CTV_rev:
  - Spatial
  - Bayesian
  - SpatioTemporal
  - Databases
  - Distributions
  - Environmetrics
  - Robust
  - TimeSeries
  landing: '2021'
  pages:
  - '101'
  - '115'
- slug: RJ-2021-061
  title: 'penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying
    Covariates'
  bibtitle: |-
    penPHcure: Variable Selection in Proportional Hazards Cure
              Model with Time-Varying Covariates
  author:
  - Alessandro Beretta
  - Cédric Heuchenne
  bibauthor: Alessandro Beretta and Cédric Heuchenne
  abstract: '  Abstract We describe the penPHcure R package, which implements the
    semiparametric proportional           hazards (PH) cure model of Sy and Taylor
    (2000) extended to time-varying covariates and the variable            selection
    technique based on its SCAD-penalized likelihood proposed by Beretta and Heuchenne            (2019a).
    In survival analysis, cure models are a useful tool when a fraction of the population
    is likely to            be immune from the event of interest. They can separate
    the effects of certain factors on the probability            of being susceptible
    and on the time until the occurrence of the event. Moreover, the penPHcure            package
    allows the user to simulate data from a PH cure model, where the event-times are
    generated            on a continuous scale from a piecewise exponential distribution
    conditional on time-varying covariates,            with a method similar to Hendry
    (2014). We present the results of a simulation study to assess the            finite
    sample performance of the methodology and illustrate the functionalities of the
    penPHcure            package using criminal recidivism data.'
  acknowledged: []
  online: '2021-06-21'
  CRANpkgs:
  - penPHcure
  - flexsurvcure
  - flexsurv
  - nltm
  - smcure
  - spduration
  - survival
  - RcmdrPlugin.survival
  CTV_rev:
  - Survival
  - ClinicalTrials
  - Distributions
  - Econometrics
  - SocialSciences
  suppl: unknown
  landing: '2021'
  pages:
  - '116'
  - '129'
- slug: RJ-2021-065
  title: 'The bdpar Package: Big Data Pipelining Architecture for R'
  bibtitle: 'The bdpar Package: Big Data Pipelining Architecture for R'
  author:
  - Miguel Ferreiro-Díaz
  - Tomás R. Cotos-Yáñez
  - José R. Méndez
  - David Ruano-Ordás
  bibauthor: |-
    Miguel Ferreiro-Díaz and Tomás R. Cotos-Yáñez and José R.
              Méndez and David Ruano-Ordás
  abstract: '  Abstract In the last years, big data has become a useful paradigm for
    taking advantage of multiple            sources to find relevant knowledge in
    real domains (such as the design of personalized marketing            campaigns
    or helping to palliate the effects of several fatal diseases). Big data programming
    tools and            methods have evolved over time from a MapReduce to a pipeline-based
    archetype. Concretely the use            of pipelining schemes has become the
    most reliable way of processing and analyzing large amounts of            data.
    To this end, this work introduces bdpar, a new highly customizable pipeline-based
    framework            (using the OOP paradigm provided by R6 package) able to execute
    multiple preprocessing tasks over            heterogeneous data sources. Moreover,
    to increase the flexibility and performance, bdpar provides            helpful
    features such as (i) the definition of a novel object-based pipe operator (%>|%),
    (ii) the ability to            easily design and deploy new (and customized) input
    data parsers, tasks, and pipelines, (iii) only-once            execution which
    avoids the execution of previously processed information (instances), guaranteeing            that
    only new both input data and pipelines are executed, (iv) the capability to perform
    serial or            parallel operations according to the user needs, (v) the
    inclusion of a debugging mechanism which            allows users to check the
    status of each instance (and find possible errors) throughout the process.'
  acknowledged: '2020-04-30'
  online: '2021-07-16'
  CRANpkgs:
  - R6
  - repo
  - drake
  - magrittr
  - rtweet
  - tuber
  - tm
  CTV_rev:
  - HighPerformanceComputing
  - ReproducibleResearch
  - WebTechnologies
  - NaturalLanguageProcessing
  suppl: unknown
  landing: '2021'
  pages:
  - '130'
  - '144'
- slug: RJ-2021-062
  title: Unidimensional and Multidimensional Methods for Recurrence Quantification
    Analysis with crqa
  bibtitle: |-
    Unidimensional and Multidimensional Methods for Recurrence
              Quantification Analysis with crqa
  author:
  - Moreno I. Coco
  - Dan Mønster
  - Giuseppe Leonardi
  - Rick Dale
  - Sebastian Wallot
  bibauthor: |-
    Moreno I. Coco and Dan Mønster and Giuseppe Leonardi and
              Rick Dale and Sebastian Wallot
  abstract: '  Abstract Recurrence quantification analysis is a widely used method
    for characterizing patterns in            time series. This article presents a
    comprehensive survey for conducting a wide range of recurrence           based
    analyses to quantify the dynamical structure of single and multivariate time series
    and capture            coupling properties underlying leader-follower relationships.
    The basics of recurrence quantification            analysis (RQA) and all its
    variants are formally introduced step-by-step from the simplest auto           recurrence
    to the most advanced multivariate case. Importantly, we show how such RQA methods
    can            be deployed under a single computational framework in R using a
    substantially renewed version of            our crqa 2.0 package. This package
    includes implementations of several recent advances in recurrence           based
    analysis, among them applications to multivariate data and improved entropy calculations            for
    categorical data. We show concrete applications of our package to example data,
    together with a            detailed description of its functions and some guidelines
    on their usage.'
  acknowledged: '2020-04-30'
  online: '2021-06-21'
  CRANpkgs:
  - tseriesChaos
  - nonlinearTseries
  - RHRV
  - crqa
  CTV_rev:
  - TimeSeries
  - Finance
  suppl: unknown
  landing: '2021'
  pages:
  - '145'
  - '163'
- slug: RJ-2021-032
  title: 'clustcurv: An R Package for Determining Groups in Multiple Curves '
  bibtitle: |-
    clustcurv: An R Package for Determining Groups in Multiple
              Curves
  author:
  - Nora M. Villanueva
  - Marta Sestelo
  - Luis Meira-Machado
  - Javier Roca-Pardiñas
  bibauthor: |-
    Nora M. Villanueva and Marta Sestelo and Luis Meira-Machado
              and Javier Roca-Pardiñas
  abstract: '  Abstract In many situations, it could be interesting to ascertain whether
    groups of curves can be            performed, especially when confronted with
    a considerable number of curves. This paper introduces            an R package,
    known as clustcurv, for determining clusters of curves with an automatic selection
    of            their number. The package can be used for determining groups in
    multiple survival curves as well as            for multiple regression curves.
    Moreover, it can be used with large numbers of curves. An illustration            of
    the use of clustcurv is provided, using both real data examples and artificial
    data.            Keywords: multiple curves, number of groups, nonparametric, survival
    analysis, regression models,            cluster'
  acknowledged: '2020-05-01'
  online: '2021-06-07'
  CRANpkgs:
  - clustcurv
  - survminer
  - npregfast
  - ggplot2
  - doParallel
  - foreach
  - condSURV
  CTV_rev:
  - Survival
  - Graphics
  - HighPerformanceComputing
  - Phylogenetics
  - TeachingStatistics
  suppl: unknown
  landing: '2021'
  pages:
  - '164'
  - '183'
- slug: RJ-2021-033
  title: Benchmarking R packages for Calculation of Persistent Homology
  bibtitle: |-
    Benchmarking R packages for Calculation of Persistent
              Homology
  author:
  - Eashwar V. Somasundaram
  - Shael E. Brown
  - Adam Litzler
  - Jacob G. Scott
  - Raoul R. Wadhwa
  bibauthor: |-
    Eashwar V. Somasundaram and Shael E. Brown and Adam Litzler
              and Jacob G. Scott and Raoul R. Wadhwa
  abstract: '  Abstract Several persistent homology software libraries have been implemented
    in R. Specifically,            the Dionysus, GUDHI, and Ripser libraries have
    been wrapped by the TDA and TDAstats CRAN            packages. These software
    represent powerful analysis tools that are computationally expensive and, to            our
    knowledge, have not been formally benchmarked. Here, we analyze runtime and memory
    growth            for the 2 R packages and the 3 underlying libraries. We find
    that datasets with less than 3 dimensions            can be evaluated with persistent
    homology fastest by the GUDHI library in the TDA package. For            higher-dimensional
    datasets, the Ripser library in the TDAstats package is the fastest. Ripser and            TDAstats
    are also the most memory-efficient tools to calculate persistent homology.'
  acknowledged: '2020-05-01'
  online: '2021-06-07'
  CRANpkgs:
  - TDA
  - TDAstats
  - readr
  - ggplot2
  - scatterplot3d
  - recexcavAAR
  - deldir
  - magick
  - bench
  - pryr
  CTV_rev:
  - Graphics
  - Multivariate
  - Phylogenetics
  - Spatial
  - TeachingStatistics
  suppl: unknown
  landing: '2021'
  pages:
  - '184'
  - '193'
- slug: RJ-2021-034
  title: Statistical Quality Control with the qcr Package
  bibtitle: Statistical Quality Control with the qcr Package
  author:
  - Miguel Flores
  - Rubén Fernández-Casal
  - Salvador Naya
  - Javier Tarrío-Saavedra
  bibauthor: |-
    Miguel Flores and Rubén Fernández-Casal and Salvador Naya
              and Javier Tarrío-Saavedra
  abstract: '      Abstract The R package qcr for Statistical Quality Control (SQC)
    is introduced and described. It                includes a comprehensive set of
    univariate and multivariate SQC tools that completes and increases                the
    SQC techniques available in R. Apart from integrating different R packages devoted
    to SQC (qcc,                MSQC), qcr provides nonparametric tools that are highly
    useful when Gaussian assumption is not                met. This package computes
    standard univariate control charts for individual measurements, x̄, S, R,                p,
    np, c, u, EWMA, and CUSUM. In addition, it includes functions to perform multivariate
    control                charts such as Hotelling T2 , MEWMA and MCUSUM. As representative
    features, multivariate                nonparametric alternatives based on data
    depth are implemented in this package: r, Q and S control                charts.
    The qcr library also estimates the most complete set of capability indices from
    first to                the fourth generation, covering the nonparametric alternatives,
    and performing the corresponding                capability analysis graphical
    outputs, including the process capability plots. Moreover, Phase I and                II
    control charts for functional data are included.                          Prácticas
    de CEC con R'
  acknowledged: '2020-05-07'
  online: '2021-06-07'
  CRANpkgs:
  - qcr
  - qcc
  - MSQC
  - IQCC
  - SixSigma
  - spcadjust
  - spc
  - MPCI
  - edcc
  suppl: unknown
  landing: '2021'
  pages:
  - '194'
  - '217'
- slug: RJ-2021-035
  title: 'pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based
    on Nonlinear Moment Conditions'
  bibtitle: |-
    pdynmc: A Package for Estimating Linear Dynamic Panel Data
              Models Based on Nonlinear Moment Conditions
  author:
  - Markus Fritsch
  - Andrew Adrian Yu Pua
  - Joachim Schnurbus
  bibauthor: |-
    Markus Fritsch and Andrew Adrian Yu Pua and Joachim
              Schnurbus
  abstract: '  Abstract This paper introduces pdynmc, an R package that provides users
    sufficient flexibility            and precise control over the estimation and
    inference in linear dynamic panel data models. The            package primarily
    allows for the inclusion of nonlinear moment conditions and the use of iterated            GMM;
    additionally, visualizations for data structure and estimation results are provided.
    The current            implementation reflects recent developments in literature,
    uses sensible argument defaults, and            aligns commercial and noncommercial
    estimation commands. Since the understanding of the model            assumptions
    is vital for setting up plausible estimation routines, we provide a broad introduction            of
    linear dynamic panel data models directed towards practitioners before concisely
    describing the            functionality available in pdynmc regarding instrument
    type, covariate type, estimation methodology,            and general configuration.
    We then demonstrate the functionality by revisiting the popular firm-level            dataset
    of Arellano and Bond (1991).'
  acknowledged: '2020-06-03'
  online: '2021-06-07'
  CRANpkgs:
  - pdynmc
  - OrthoPanels
  - plm
  - panelvar
  - optimx
  CTV_rev:
  - Econometrics
  - Optimization
  - SpatioTemporal
  landing: '2021'
  pages:
  - '218'
  - '231'
- slug: RJ-2021-036
  title: 'DChaos: An R Package for Chaotic Time Series Analysis'
  bibtitle: 'DChaos: An R Package for Chaotic Time Series Analysis'
  author:
  - Julio E. Sandubete
  - Lorenzo Escot
  bibauthor: Julio E. Sandubete and Lorenzo Escot
  abstract: '  Abstract Chaos theory has been hailed as a revolution of thoughts and
    attracting ever-increasing            attention of many scientists from diverse
    disciplines. Chaotic systems are non-linear deterministic            dynamic systems
    which can behave like an erratic and apparently random motion. A relevant field            inside
    chaos theory is the detection of chaotic behavior from empirical time-series data.
    One of the            main features of chaos is the well-known initial-value sensitivity
    property. Methods and techniques            related to testing the hypothesis
    of chaos try to quantify the initial-value sensitive property estimating            the
    so-called Lyapunov exponents. This paper describes the main estimation methods
    of the Lyapunov            exponent from time series data. At the same time, we
    present the DChaos library. R users may            compute the delayed-coordinate
    embedding vector from time series data, estimates the best-fitted            neural
    net model from the delayed-coordinate embedding vectors, calculates analytically
    the partial            derivatives from the chosen neural nets model. They can
    also obtain the neural net estimator of the            Lyapunov exponent from
    the partial derivatives computed previously by two different procedures            and
    four ways of subsampling by blocks. To sum up, the DChaos package allows the R
    users to test            robustly the hypothesis of chaos in order to know if
    the data-generating process behind time series            behaves chaotically
    or not. The package’s functionality is illustrated by examples.'
  acknowledged: '2020-06-03'
  online: '2021-06-07'
  CRANpkgs:
  - tseriesChaos
  - nonlinearTseries
  - fNonlinear
  - DChaos
  - highfrequency
  - nnet
  CTV_rev:
  - TimeSeries
  - Finance
  - Econometrics
  - MachineLearning
  - SocialSciences
  suppl: unknown
  landing: '2021'
  pages:
  - '232'
  - '252'
- slug: RJ-2021-038
  title: 'IndexNumber: An R Package for Measuring the Evolution of Magnitudes'
  bibtitle: |-
    IndexNumber: An R Package for Measuring the Evolution of
              Magnitudes
  author:
  - Alejandro Saavedra-Nieves
  - Paula Saavedra-Nieves
  bibauthor: Alejandro Saavedra-Nieves and Paula Saavedra-Nieves
  abstract: '  Abstract Index numbers are descriptive statistical measures useful
    in economic settings for comparing            simple and complex magnitudes registered,
    usually in two time periods. Although this theory has            a large history,
    it still plays an important role in modern today’s societies where big amounts
    of            economic data are available and need to be analyzed. After a detailed
    revision on classical index            numbers in literature, this paper is focused
    on the description of the R package IndexNumber with            strong capabilities
    for calculating them. Two of the four real data sets contained in this library            are
    used for illustrating the determination of the index numbers in this work. Graphical
    tools are            also implemented in order to show the time evolution of considered
    magnitudes simplifying the            interpretation of the results.'
  acknowledged: '2020-06-03'
  online: '2021-06-07'
  CRANpkgs: IndexNumber
  suppl: unknown
  landing: '2021'
  pages:
  - '253'
  - '275'
- slug: RJ-2021-057
  title: 'garchx: Flexible and Robust GARCH-X Modeling'
  bibtitle: 'garchx: Flexible and Robust GARCH-X Modeling'
  author: Genaro Sucarrat
  bibauthor: Genaro Sucarrat
  abstract: '  Abstract The garchx package provides a user-friendly, fast, flexible,
    and robust framework for the            estimation and inference of GARCH(p, q,
    r)-X models, where p is the ARCH order, q is the GARCH            order, r is
    the asymmetry or leverage order, and ’X’ indicates that covariates can be included.
    Quasi            Maximum Likelihood (QML) methods ensure estimates are consistent
    and standard errors valid,            even when the standardized innovations are
    non-normal or dependent, or both. Zero-coefficient            restrictions by
    omission enable parsimonious specifications, and functions to facilitate the non-standard            inference
    associated with zero-restrictions in the null-hypothesis are provided. Finally,
    in the formal            comparisons of precision and speed, the garchx package
    performs well relative to other prominent            GARCH-packages on CRAN.'
  acknowledged: '2020-06-03'
  online: '2021-06-22'
  CRANpkgs:
  - tseries
  - fGarch
  - rugarch
  - xts
  - zoo
  - microbenchmark
  CTV_rev:
  - Finance
  - TimeSeries
  - Econometrics
  - Environmetrics
  - MissingData
  - SpatioTemporal
  suppl: unknown
  landing: '2021'
  pages:
  - '276'
  - '291'
- slug: RJ-2021-040
  title: 'ROBustness In Network (robin): an R Package for Comparison and Validation
    of Communities '
  bibtitle: |-
    ROBustness In Network (robin): an R Package for Comparison
              and Validation of Communities
  author:
  - Valeria Policastro
  - Dario Righelli
  - Annamaria Carissimo
  - Luisa Cutillo
  - Italia De Feis
  bibauthor: |-
    Valeria Policastro and Dario Righelli and Annamaria
              Carissimo and Luisa Cutillo and Italia De Feis
  abstract: '  Abstract In network analysis, many community detection algorithms have
    been developed. However,            their implementation leaves unaddressed the
    question of the statistical validation of the results. Here,            we present
    robin (ROBustness In Network), an R package to assess the robustness of the community            structure
    of a network found by one or more methods to give indications about their reliability.
    The            procedure initially detects if the community structure found by
    a set of algorithms is statistically            significant and then compares
    two selected detection algorithms on the same graph to choose the            one
    that better fits the network of interest. We demonstrate the use of our package
    on the American            College Football benchmark dataset.'
  acknowledged: '2020-06-03'
  online: '2021-06-07'
  CRANpkgs:
  - robin
  - igraph
  - networkD3
  - ggplot2
  - gridExtra
  - fdatest
  - DescTools
  BIOpkgs: gprege
  CTV_rev:
  - Graphics
  - FunctionalData
  - gR
  - MissingData
  - Optimization
  - Phylogenetics
  - Spatial
  - TeachingStatistics
  landing: '2021'
  pages:
  - '292'
  - '309'
- slug: RJ-2021-041
  title: Finding Optimal Normalizing Transformations via bestNormalize
  bibtitle: |-
    Finding Optimal Normalizing Transformations via
              bestNormalize
  author: Ryan A. Peterson
  bibauthor: Ryan A. Peterson
  abstract: ' Abstract The bestNormalize R package was designed to help users find
    a transformation that can           effectively normalize a vector regardless
    of its actual distribution. Each of the many normalization           techniques
    that have been developed has its own strengths and weaknesses, and deciding which
    to           use until data are fully observed is difficult or impossible. This
    package facilitates choosing between           a range of possible transformations
    and will automatically return the best one, i.e., the one that           makes
    data look the most normal. To evaluate and compare the normalization efficacy
    across a suite           of possible transformations, we developed a statistic
    based on a goodness of fit test divided by its           degrees of freedom. Transformations
    can be seamlessly trained and applied to newly observed data           and can
    be implemented in conjunction with caret and recipes for data preprocessing in
    machine           learning workflows. Custom transformations and normalization
    statistics are supported.'
  acknowledged: '2020-06-03'
  online: '2021-06-07'
  CRANpkgs:
  - bestNormalize
  - caret
  - recipes
  - MASS
  - LambertW
  - nortest
  - parallel
  - doRNG
  - tidymodels
  - visreg
  - scales
  - ggplot2
  - mgcv
  - yardstick
  CTV_rev:
  - SocialSciences
  - Distributions
  - Econometrics
  - Environmetrics
  - HighPerformanceComputing
  - Multivariate
  - TeachingStatistics
  - Bayesian
  - Graphics
  - MachineLearning
  - NumericalMathematics
  - Phylogenetics
  - Psychometrics
  - Robust
  suppl: unknown
  landing: '2021'
  pages:
  - '310'
  - '329'
- slug: RJ-2021-042
  title: Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured
    Models
  bibtitle: |-
    Package wsbackfit for Smooth Backfitting Estimation of
              Generalized Structured Models
  author:
  - Javier Roca-Pardiñas
  - María Xosé Rodríguez-Álvarez
  - Stefan Sperlich
  bibauthor: |-
    Javier Roca-Pardiñas and María Xosé Rodríguez-Álvarez and
              Stefan Sperlich
  abstract: ' Abstract A package is introduced that provides the weighted smooth backfitting
    estimator for           a large family of popular semiparametric regression models.
    This family is known as generalized           structured models, comprising, for
    example, generalized varying coefficient model, generalized           additive
    models, mixtures, potentially including parametric parts. The kernel-based weighted           smooth
    backfitting belongs to the statistically most efficient procedures for this model
    class. Its           asymptotic properties are well-understood thanks to the large
    body of literature about this estimator.           The introduced weights allow
    for the inclusion of sampling weights, trimming, and efficient estimation           under
    heteroscedasticity. Further options facilitate easy handling of aggregated data,
    prediction,           and the presentation of estimation results. Cross-validation
    methods are provided which can be used           for model and bandwidth selection.1'
  acknowledged: []
  online: '2021-06-07'
  CRANpkgs:
  - sBF
  - wsbackfit
  - BayesX
  - gam
  - mgcv
  - GAMLSS
  - GAMBoost
  - np
  CTV_rev:
  - Econometrics
  - SocialSciences
  - Bayesian
  - Environmetrics
  suppl: unknown
  landing: '2021'
  pages:
  - '330'
  - '350'
- slug: RJ-2021-043
  title: 'RLumCarlo: Simulating Cold Light using Monte Carlo Methods'
  bibtitle: 'RLumCarlo: Simulating Cold Light using Monte Carlo Methods'
  author:
  - Sebastian Kreutzer
  - Johannes Friedrich
  - Vasilis Pagonis
  - Christian Laag
  - Ena Rajovic
  - '           Christoph Schmidt'
  bibauthor: |-
    Sebastian Kreutzer and Johannes Friedrich and Vasilis
              Pagonis and Christian Laag and Ena Rajovic and
              Christoph Schmidt
  abstract: '  Abstract The luminescence phenomena of insulators and semiconductors
    (e.g., natural minerals such            as quartz) have various application domains.
    For instance, Earth Sciences and archaeology exploit            luminescence as
    a dating method. Herein, we present the R package RLumCarlo implementing sets
    of            luminescence models to be simulated with Monte Carlo (MC) methods.
    MC methods make a powerful            ally to all kinds of simulation attempts
    involving stochastic processes. Luminescence production            is such a stochastic
    process in the form of charge (electron-hole pairs) interaction within insulators            and
    semiconductors. To simulate luminescence-signal curves, we distribute single and
    independent            MC processes to virtual MC clusters. RLumCarlo comes with
    a modularized design and consistent            user interface: (1) C++ functions
    represent the modeling core and implement models for specific            stimulations
    modes. (2) R functions give access to combinations of models and stimulation modes,            start
    the simulation and render terminal and graphical feedback. The combination of
    MC clusters            supports the simulation of complex luminescence phenomena.'
  acknowledged: []
  online: '2021-06-07'
  CRANpkgs:
  - RLumCarlo
  - BayLum
  - Luminescence
  - numOSL
  - RLumModel
  - RLumShiny
  - tgcd
  - scales
  - ggplot2
  - Rcpp
  - parallel
  - doParallel
  - foreach
  CTV_rev:
  - HighPerformanceComputing
  - Graphics
  - NumericalMathematics
  - Phylogenetics
  - TeachingStatistics
  suppl: unknown
  landing: '2021'
  pages:
  - '351'
  - '365'
- slug: RJ-2021-044
  title: 'OneStep : Le Cam''s One-step Estimation Procedure'
  bibtitle: 'OneStep : Le Cam''s One-step Estimation Procedure'
  author:
  - Alexandre Brouste
  - Christophe Dutang
  - Darel Noutsa Mieniedou
  bibauthor: |-
    Alexandre Brouste and Christophe Dutang and Darel Noutsa
              Mieniedou
  abstract: '  Abstract The OneStep package proposes principally an eponymic function
    that numerically computes            Le Cam’s one-step estimator, which is asymptotically
    efficient and can be computed faster than the            maximum likelihood estimator
    for large datasets. Monte Carlo simulations are carried out for several            examples
    (discrete and continuous probability distributions) in order to exhibit the performance
    of Le            Cam’s one-step estimation procedure in terms of efficiency and
    computational cost on observation            samples of finite size.'
  acknowledged: '2020-10-27'
  online: '2021-06-08'
  suppl: unknown
  landing: '2021'
  pages:
  - '366'
  - '377'
- slug: RJ-2021-059
  title: The HBV.IANIGLA Hydrological Model
  bibtitle: The HBV.IANIGLA Hydrological Model
  author:
  - Ezequiel Toum
  - Mariano H. Masiokas
  - Ricardo Villalba
  - Pierre Pitte
  - Lucas Ruiz
  bibauthor: |-
    Ezequiel Toum and Mariano H. Masiokas and Ricardo Villalba
              and Pierre Pitte and Lucas Ruiz
  abstract: '  Abstract Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning)
    hydrological            model has been one of the most used worldwide due to its
    robustness, simplicity, and reliable results.            Despite these advantages,
    the available versions impose some limitations for research studies in            mountain
    watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited
    number of            elevation bands, among other constraints). Here we present
    HBV.IANIGLA, a tool for hydroclimatic            studies in regions with steep
    topography and/or cryospheric processes which provides a modular            and
    extended implementation of the HBV model as an R package. To our knowledge, this
    is the first            modular version of the original HBV model. This feature
    can be very useful for teaching hydrological            modeling, as it offers
    the possibility to build a customized, open-source model that can be adjusted
    to            different requirements of students and users.'
  acknowledged: '2020-10-27'
  online: '2021-06-21'
  CRANpkgs:
  - TUWmodel
  - topmodel
  - dynatopmodel
  - airGR
  - glacierSMBM
  - HBV.IANIGLA
  - Evapotranspiration
  - Rcpp
  - microbechmark
  - DEoptim
  - tidyverse
  - sp
  - raster
  - hydroGOF
  - plotly
  - hydroToolkit
  CTV_rev:
  - Hydrology
  - Environmetrics
  - Spatial
  - SpatioTemporal
  - HighPerformanceComputing
  - NumericalMathematics
  - Optimization
  - WebTechnologies
  suppl: unknown
  landing: '2021'
  pages:
  - '378'
  - '395'
- slug: RJ-2021-045
  title: 'The R Package smicd: Statistical Methods for Interval-Censored Data'
  bibtitle: |-
    The R Package smicd: Statistical Methods for Interval-
              Censored Data
  author: Paul Walter
  bibauthor: Paul Walter
  abstract: '  Abstract The package allows the use of two new statistical methods
    for the analysis of interval           censored data: 1) direct estimation/prediction
    of statistical indicators and 2) linear (mixed) regression            analysis.
    Direct estimation of statistical indicators, for instance, poverty and inequality
    indicators,            is facilitated by a non parametric kernel density algorithm.
    The algorithm is able to account for            weights in the estimation of statistical
    indicators. The standard errors of the statistical indicators are            estimated
    with a non parametric bootstrap. Furthermore, the package offers statistical methods
    for            the estimation of linear and linear mixed regression models with
    an interval-censored dependent            variable, particularly random slope
    and random intercept models. Parameter estimates are obtained            through
    a stochastic expectation-maximization algorithm. Standard errors are estimated
    using a            non parametric bootstrap in the linear regression model and
    by a parametric bootstrap in the linear            mixed regression model. To
    handle departures from the model assumptions, fixed (logarithmic) and            data-driven
    (Box-Cox) transformations are incorporated into the algorithm.'
  acknowledged: []
  online: '2021-06-08'
  CRANpkgs:
  - actuar
  - fitdistrplus
  - smicd
  - stats
  - MASS
  - survival
  - lme4
  - nlme
  - ordinal
  - laeken
  - mlmRev
  CTV_rev:
  - Econometrics
  - Psychometrics
  - SocialSciences
  - Distributions
  - Environmetrics
  - OfficialStatistics
  - Finance
  - SpatioTemporal
  - Survival
  - ChemPhys
  - ClinicalTrials
  - Multivariate
  - NumericalMathematics
  - Robust
  - Spatial
  - TeachingStatistics
  landing: '2021'
  pages:
  - '396'
  - '412'
- slug: RJ-2021-046
  title: 'krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff''s
    Alpha Coefficient'
  bibtitle: |-
    krippendorffsalpha: An R Package for Measuring Agreement
              Using Krippendorff's Alpha Coefficient
  author: John Hughes
  bibauthor: John Hughes
  abstract: '  Abstract R package krippendorffsalpha provides tools for measuring
    agreement using Krippendorff’s            α coefficient, a well-known nonparametric
    measure of agreement (also called inter-rater reliability            and various
    other names). This article first develops Krippendorff’s α in a natural way and
    situates            α among statistical procedures. Then, the usage of package
    krippendorffsalpha is illustrated via            analyses of two datasets, the
    latter of which was collected during an imaging study of hip cartilage.            The
    package permits users to apply the α methodology using built-in distance functions
    for the            nominal, ordinal, interval, or ratio levels of measurement.
    User-defined distance functions are also            supported. The fitting function
    can accommodate any number of units, any number of coders, and            missingness.
    Bootstrap inference is supported, and the bootstrap computation can be carried
    out in            parallel.'
  acknowledged: '2020-10-30'
  online: '2021-06-08'
  CRANpkgs:
  - krippendorffsalpha
  - irr
  - icr
  - irrCAC
  - pbapply
  CTV_rev:
  - HighPerformanceComputing
  - Psychometrics
  suppl: unknown
  landing: '2021'
  pages:
  - '413'
  - '425'
- slug: RJ-2021-047
  title: 'Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing'
  bibtitle: |-
    Working with CRSP/COMPUSTAT in R: Reproducible Empirical
              Asset Pricing
  author: Majeed Simaan
  bibauthor: Majeed Simaan
  abstract: '  Abstract It is common to come across SAS or Stata manuals while working
    on academic empirical            finance research. Nonetheless, given the popularity
    of open-source programming languages such as R,            there are fewer resources
    in R covering popular databases such as CRSP and COMPUSTAT. The aim            of
    this article is to bridge the gap and illustrate how to leverage R in working
    with both datasets. As            an application, we illustrate how to form size-value
    portfolios with respect to Fama and French (1993)            and study the sensitivity
    of the results with respect to different inputs. Ultimately, the purpose of the            article
    is to advocate reproducible finance research and contribute to the recent idea
    of “Open Source            Cross-Sectional Asset Pricing”, proposed by Chen and
    Zimmermann (2020).'
  acknowledged: '2020-10-30'
  online: '2021-06-08'
  CRANpkgs:
  - data.table
  - lubridate
  - ggplot2
  - parallel
  - plyr
  - dplyr
  - rollRegres
  CTV_rev:
  - TimeSeries
  - Databases
  - Finance
  - Graphics
  - HighPerformanceComputing
  - ModelDeployment
  - Phylogenetics
  - ReproducibleResearch
  - TeachingStatistics
  suppl: unknown
  landing: '2021'
  pages:
  - '426'
  - '443'
- slug: RJ-2021-055
  title: 'distr6: R6 Object-Oriented Probability Distributions Interface in R'
  bibtitle: |-
    distr6: R6 Object-Oriented Probability Distributions
              Interface in R
  author:
  - Raphael Sonabend
  - Franz J. Király
  bibauthor: Raphael Sonabend and Franz J. Király
  abstract: '  Abstract distr6 is an object-oriented (OO) probability distributions
    interface leveraging the extensibil           ity and scalability of R6 and the
    speed and efficiency of Rcpp. Over 50 probability distributions are            currently
    implemented in the package with ‘core’ methods, including density, distribution,
    and gener           ating functions, and more ‘exotic’ ones, including hazards
    and distribution function anti-derivatives.            In addition to simple distributions,
    distr6 supports compositions such as truncation, mixtures, and            product
    distributions. This paper presents the core functionality of the package and demonstrates            examples
    for key use-cases. In addition, this paper provides a critical review of the object-oriented            programming
    paradigms in R and describes some novel implementations for design patterns and
    core            object-oriented features introduced by the package for supporting
    distr6 components.'
  acknowledged: '2020-10-30'
  online: '2021-06-17'
  CRANpkgs:
  - distr6
  - distr
  - R6
  - extraDistr
  - actuar
  - distributions3
  - distributional
  - mistr
  - R62S3
  - Rcpp
  - set6
  - microbenchmark
  CTV_rev:
  - Distributions
  - NumericalMathematics
  - Finance
  - HighPerformanceComputing
  - Robust
  suppl: unknown
  landing: '2021'
  pages:
  - '444'
  - '466'
- slug: RJ-2021-060
  title: 'gofCopula: Goodness-of-Fit Tests for Copulae'
  bibtitle: 'gofCopula: Goodness-of-Fit Tests for Copulae'
  author:
  - Ostap Okhrin
  - Simon Trimborn
  - Martin Waltz
  bibauthor: Ostap Okhrin and Simon Trimborn and Martin Waltz
  abstract: '  Abstract The last decades show an increased interest in modeling various
    types of data through            copulae. Different copula models have been developed,
    which lead to the challenge of finding the            best fitting model for a
    particular dataset. From the other side, a strand of literature developed a list            of
    different Goodness-of-Fit (GoF) tests with different powers under different conditions.
    The usual            practice is the selection of the best copula via the p-value
    of the GoF test. Although this method is not            purely correct due to
    the fact that non-rejection does not imply acception, this strategy is favored
    by            practitioners. Unfortunately, different GoF tests often provide
    contradicting outputs. The proposed            R-package brings under one umbrella
    13 most used copulae plus their rotated variants together            with 16 GoF
    tests and a hybrid one. The package offers flexible margin modeling, automatized            parallelization,
    parameter estimation, as well as a user-friendly interface, and pleasant visualizations            of
    the results. To illustrate the functionality of the package, two exemplary applications
    are provided.'
  acknowledged: '2020-10-30'
  online: '2021-06-22'
  CRANpkgs:
  - copula
  - TwoCop
  - VineCopula
  - gofCopula
  - progress
  - yarrr
  CTV_rev:
  - Distributions
  - ExtremeValue
  - Finance
  - Multivariate
  landing: '2021'
  pages:
  - '467'
  - '498'
- slug: RJ-2021-049
  title: Analyzing Dependence between Point Processes in Time Using IndTestPP
  bibtitle: |-
    Analyzing Dependence between Point Processes in Time Using
              IndTestPP
  author:
  - Ana C. Cebrián
  - Jesús Asín
  bibauthor: Ana C. Cebrián and Jesús Asín
  abstract: '  Abstract The need to analyze the dependence between two or more point
    processes in time appears in            many modeling problems related to the
    occurrence of events, such as the occurrence of climate events            at different
    spatial locations or synchrony detection in spike train analysis. The package
    IndTestPP            provides a general framework for all the steps in this type
    of analysis, and one of its main features is the            implementation of
    three families of tests to study independence given the intensities of the processes,            which
    are not only useful to assess independence but also to identify factors causing
    dependence.            The package also includes functions for generating different
    types of dependent point processes,            and implements computational statistical
    inference tools using them. An application to characterize            the dependence
    between the occurrence of extreme heat events in three Spanish locations using
    the            package is shown.'
  acknowledged: '2020-10-30'
  online: '2021-06-08'
  CRANpkgs:
  - IndTestPP
  - spatstat
  - stpp
  - splancs
  - IDSpatialStats
  - NHPoisson
  - PtProcess
  - mmpp
  - mppa
  - stats
  CTV_rev:
  - Spatial
  - SpatioTemporal
  - Survival
  suppl: unknown
  landing: '2021'
  pages:
  - '499'
  - '515'
- slug: RJ-2021-050
  title: 'Conversations in Time: Interactive Visualization to Explore Structured Temporal
    Data'
  bibtitle: |-
    Conversations in Time: Interactive Visualization to Explore
              Structured Temporal Data
  author:
  - Earo Wang
  - Dianne Cook
  bibauthor: Earo Wang and Dianne Cook
  abstract: '  Abstract Temporal data often has a hierarchical structure, defined
    by categorical variables describing            different levels, such as political
    regions or sales products. The nesting of categorical variables            produces
    a hierarchical structure. The tsibbletalk package is developed to allow a user
    to interactively            explore temporal data, relative to the nested or crossed
    structures. It can help to discover differences            between category levels,
    and uncover interesting periodic or aperiodic slices. The package implements            a
    shared tsibble object that allows for linked brushing between coordinated views,
    and a shiny            module that aids in wrapping timelines for seasonal patterns.
    The tools are demonstrated using two            data examples: domestic tourism
    in Australia and pedestrian traffic in Melbourne.'
  acknowledged: '2020-10-30'
  online: '2021-06-08'
  CRANpkgs:
  - tsibbletalk
  - tsibble
  - tsibbledata
  - tidyverse
  - feasts
  - fable
  - shiny
  - htmlwidgets
  - plotly
  - rbokeh
  - leaflet
  - crosstalk
  - grDevices
  - loon
  - ggplot2
  CTV_rev:
  - TimeSeries
  - TeachingStatistics
  - WebTechnologies
  - Graphics
  - MissingData
  - Phylogenetics
  - Spatial
  suppl: unknown
  landing: '2021'
  pages:
  - '516'
  - '524'
- slug: RJ-2021-066
  title: 'ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference
    With and Without Covariates'
  bibtitle: |-
    ROCnReg: An R Package for Receiver Operating Characteristic
              Curve Inference With and Without Covariates
  author:
  - María Xosé Rodríguez-Álvarez
  - Vanda Inácio
  bibauthor: María Xosé Rodríguez-Álvarez and Vanda Inácio
  abstract: '  Abstract This paper introduces the package ROCnReg that allows estimating
    the pooled ROC            curve, the covariate-specific ROC curve, and the covariate-adjusted
    ROC curve by different methods,            both from (semi) parametric and nonparametric
    perspectives and within Bayesian and frequentist            paradigms. From the
    estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several            summary
    measures of discriminatory accuracy, such as the (partial) area under the ROC
    curve and the            Youden index, can be obtained. The package also provides
    functions to obtain ROC-based optimal            threshold values using several
    criteria, namely, the Youden index criterion and the criterion that            sets
    a target value for the false positive fraction. For the Bayesian methods, we provide
    tools for            assessing model fit via posterior predictive checks, while
    the model choice can be carried out via            several information criteria.
    Numerical and graphical outputs are provided for all methods. This is            the
    only package implementing Bayesian procedures for ROC curves.'
  acknowledged: '2020-10-30'
  online: '2021-07-15'
  CRANpkgs:
  - sROC
  - pROC
  - nsROC
  - npROCRegression
  - OptimalCutpoints
  - ThresholdROC
  - ROCnReg
  - ggplot2
  - coda
  - moments
  - nor1mix
  - Matrix
  - spatstat
  - np
  - lattice
  - MASS
  - pbivnorm
  CTV_rev:
  - Distributions
  - Econometrics
  - Multivariate
  - Graphics
  - NumericalMathematics
  - SocialSciences
  - TeachingStatistics
  - Bayesian
  - Cluster
  - Environmetrics
  - gR
  - Phylogenetics
  - Psychometrics
  - Robust
  - Spatial
  - SpatioTemporal
  - Survival
  suppl: unknown
  landing: '2021'
  pages:
  - '525'
  - '555'
- slug: RJ-2021-051
  title: Automating Reproducible, Collaborative Clinical Trial Document Generation
    with the listdown Package
  bibtitle: |-
    Automating Reproducible, Collaborative Clinical Trial
              Document Generation with the listdown Package
  author:
  - Michael Kane
  - Xun Jiang
  - Simon Urbanek
  bibauthor: Michael Kane and Xun Jiang and Simon Urbanek
  abstract: '  Abstract The conveyance of clinical trial explorations and analysis
    results from a statistician to a clinical            investigator is a critical
    component of the drug development and clinical research cycle. Automating            the
    process of generating documents for data descriptions, summaries, exploration,
    and analysis            allows the statistician to provide a more comprehensive
    view of the information captured by a clinical            trial, and efficient
    generation of these documents allows the statistican to focus more on the conceptual            development
    of a trial or trial analysis and less on the implementation of the summaries and
    results            on which decisions are made. This paper explores the use of
    the listdown package for automating            reproducible documents in clinical
    trials that facilitate the collaboration between statisticians and            clinicians
    as well as defining an analysis pipeline for document generation.'
  acknowledged: '2020-11-02'
  online: '2021-06-08'
  suppl: unknown
  landing: '2021'
  pages:
  - '556'
  - '562'
- slug: RJ-2021-052
  title: Towards a Grammar for Processing Clinical Trial Data
  bibtitle: Towards a Grammar for Processing Clinical Trial Data
  author: Michael J. Kane
  bibauthor: Michael J. Kane
  abstract: '  Abstract The goal of this paper is to help define a path toward a grammar
    for processing clinical            trials by a) defining a format in which we
    would like to represent data from standardized clinical            trial data
    b) describing a standard set of operations to transform clinical trial data into
    this format,            and c) to identify a set of verbs and other functionality
    to facilitate data processing and encourage            reproducibility in the
    processing of these data. It provides a background on standard clinical trial
    data            and goes through a simple preprocessing example illustrating the
    value of the proposed approach            through the use of the forceps package,
    which is currently being used for data of this kind.'
  acknowledged: '2020-11-02'
  online: '2021-06-08'
  suppl: unknown
  landing: '2021'
  pages:
  - '563'
  - '569'
- slug: RJ-2021-053
  title: Reproducible Summary Tables with the gtsummary Package
  bibtitle: Reproducible Summary Tables with the gtsummary Package
  author:
  - Daniel D. Sjoberg
  - Karissa Whiting
  - Michael Curry
  - Jessica A. Lavery
  - Joseph Larmarange
  bibauthor: |-
    Daniel D. Sjoberg and Karissa Whiting and Michael Curry and
              Jessica A. Lavery and Joseph Larmarange
  abstract: '  Abstract The gtsummary package provides an elegant and flexible way
    to create publication-ready            summary tables in R. A critical part of
    the work of statisticians, data scientists, and analysts is            summarizing
    data sets and regression models in R and publishing or sharing polished summary
    tables.            The gtsummary package was created to streamline these everyday
    analysis tasks by allowing users            to easily create reproducible summaries
    of data sets, regression models, survey data, and survival            data with
    a simple interface and very little code. The package follows a tidy framework,
    making it            easy to integrate with standard data workflows, and offers
    many table customization features through            function arguments, helper
    functions, and custom themes.'
  acknowledged: '2021-01-15'
  online: '2021-06-22'
  CRANpkgs:
  - gtsummary
  - broom
  - gt
  - magrittr
  - tidyselect
  - tidyverse
  - skimr
  - stargazer
  - finalfit
  - tableone
  - glue
  - survey
  - parameters
  - broom.helpers
  - flextable
  - huxtable
  - kableExtra
  CTV_rev:
  - ReproducibleResearch
  - Distributions
  - OfficialStatistics
  - SocialSciences
  - Survival
  - WebTechnologies
  suppl: unknown
  landing: '2021'
  pages:
  - '570'
  - '580'
- slug: RJ-2021-054
  title: 'Regularized Transformation Models: The tramnet Package'
  bibtitle: 'Regularized Transformation Models: The tramnet Package'
  author:
  - Lucas Kook
  - Torsten Hothorn
  bibauthor: Lucas Kook and Torsten Hothorn
  abstract: '  Abstract The tramnet package implements regularized linear transformation
    models by combining the            flexible class of transformation models from
    tram with constrained convex optimization implemented            in CVXR. Regularized
    transformation models unify many existing and novel regularized regression            models
    under one theoretical and computational framework. Regularization strategies implemented            for
    transformation models in tramnet include the Lasso, ridge regression, and the
    elastic net and            follow the parameterization in glmnet. Several functionalities
    for optimizing the hyperparameters,            including model-based optimization
    based on the mlrMBO package, are implemented. A multitude            of S3 methods
    is deployed for visualization, handling, and simulation purposes. This work aims
    at            illustrating all facets of tramnet in realistic settings and comparing
    regularized transformation models            with existing implementations of
    similar models.'
  acknowledged: '2021-01-15'
  online: '2021-06-08'
  CRANpkgs:
  - tramnet
  - tram
  - CVXR
  - glmnet
  - mlrMBO
  - penalized
  - basefun
  - mlt
  - coin
  - trtf
  - tbm
  CTV_rev:
  - MachineLearning
  - Survival
  - Optimization
  - ClinicalTrials
  suppl: unknown
  landing: '2021'
  pages:
  - '581'
  - '594'
- slug: RJ-2021-068
  title: 'BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population
    Survival Models'
  bibtitle: |-
    BayesSPsurv: An R Package to Estimate Bayesian (Spatial)
              Split-Population Survival Models
  author:
  - Brandon Bolte
  - Nicolás Schmidt
  - Sergio Béjar
  - Nguyen Huynh
  - Bumba Mukherjee
  bibauthor: |-
    Brandon Bolte and Nicolás Schmidt and Sergio Béjar and
              Nguyen Huynh and Bumba Mukherjee
  abstract: '  Abstract Survival data often include a fraction of units that are susceptible
    to an event of interest            as well as a fraction of “immune” units. In
    many applications, spatial clustering in unobserved risk            factors across
    nearby units can also affect their survival rates and odds of becoming immune.
    To            address these methodological challenges, this article introduces
    our BayesSPsurv R-package, which            fits parametric Bayesian Spatial split-population
    survival (cure) models that can account for spatial            autocorrelation
    in both subpopulations of the user’s time-to-event data. Spatial autocorrelation
    is            modeled with spatially weighted frailties, which are estimated using
    a conditionally autoregressive            prior. The user can also fit parametric
    cure models with or without nonspatial i.i.d. frailties, and            each model
    can incorporate time-varying covariates. BayesSPsurv also includes various functions
    to            conduct pre-estimation spatial autocorrelation tests, visualize
    results, and assess model performance,            all of which are illustrated
    using data on post-civil war peace survival.'
  acknowledged: '2021-02-01'
  online: '2021-07-15'
  CRANpkgs:
  - BayesSPsurv
  - survival
  - dynsurv
  - smcure
  - nltm
  - flexsurvcure
  - spduration
  - BayesX
  - R2BayesX
  - spBayesSurv
  - spatsurv
  - Rcpp
  - coda
  - doParallel
  - doRNG
  - countrycode
  - rworldmap
  CTV_rev:
  - Survival
  - Bayesian
  - HighPerformanceComputing
  - Spatial
  - ClinicalTrials
  - Econometrics
  - gR
  - NumericalMathematics
  - OfficialStatistics
  - SocialSciences
  landing: '2021'
  pages:
  - '595'
  - '613'
- slug: RJ-2021-063
  title: 'stratamatch: Prognostic Score Stratification Using a Pilot Design'
  bibtitle: |-
    stratamatch: Prognostic Score Stratification Using a Pilot
              Design
  author:
  - Rachael C. Aikens
  - Joseph Rigdon
  - Justin Lee
  - Michael Baiocchi
  - Andrew B. Goldstone
  - Peter Chiu
  - '          Y. Joseph Woo'
  - Jonathan H. Chen
  bibauthor: |-
    Rachael C. Aikens and Joseph Rigdon and Justin Lee and
              Michael Baiocchi and Andrew B. Goldstone and Peter
              Chiu and Y. Joseph Woo and Jonathan H. Chen
  abstract: '  Abstract Optimal propensity score matching has emerged as one of the
    most ubiquitous approaches            for causal inference studies on observational
    data. However, outstanding critiques of the statistical            properties
    of propensity score matching have cast doubt on the statistical efficiency of
    this technique,            and the poor scalability of optimal matching to large
    data sets makes this approach inconvenient            if not infeasible for sample
    sizes that are increasingly commonplace in modern observational data.            The
    stratamatch package provides implementation support and diagnostics for ‘stratified
    matching            designs,’ an approach that addresses both of these issues
    with optimal propensity score matching for            large-sample observational
    studies. First, stratifying the data enables more computationally efficient            matching
    of large data sets. Second, stratamatch implements a ‘pilot design’ approach in
    order to            stratify by a prognostic score, which may increase the precision
    of the effect estimate and increase            power in sensitivity analyses of
    unmeasured confounding.'
  acknowledged: '2021-03-01'
  online: '2021-06-21'
  CRANpkgs:
  - stratamatch
  - MatchIt
  - optmatch
  - DOS2
  - nearfar
  - sensitivitymw
  - sensitivityfull
  - glmnet
  CTV_rev:
  - SocialSciences
  - MachineLearning
  - OfficialStatistics
  - Optimization
  - Survival
  suppl: unknown
  landing: '2021'
  pages:
  - '614'
  - '630'
- heading: News and Notes
  pages:
  - '614'
  - '630'
- slug: core
  author:
  - Tomas Kalibera
  - Sebastian Meyer
  - Kurt Hornik
  title: Changes in R 4.0–4.1
  bibtitle: Changes in R 4.0–4.1
  bibauthor: Tomas Kalibera and Sebastian Meyer and Kurt Hornik
  pages:
  - '631'
  - '633'
- slug: cran
  author:
  - Kurt Hornik
  - Uwe Ligges
  - Achim Zeileis
  title: Changes on CRAN
  bibtitle: Changes on CRAN
  bibauthor: Kurt Hornik and Uwe Ligges and Achim Zeileis
  pages:
  - '634'
  - '636'
- slug: bioc
  author: Bioconductor Core Team
  title: News from the Bioconductor Project
  bibtitle: News from the Bioconductor Project
  bibauthor: Bioconductor Core Team
  pages:
  - '637'
  - '638'
- slug: forwards-news
  author: Heather Turner
  title: News from the Forwards Taskforce
  bibtitle: News from the Forwards Taskforce
  bibauthor: Heather Turner
  pages:
  - '639'
  - '639'
- slug: foundation
  author: Torsten Hothorn
  title: R Foundation News
  bibtitle: R Foundation News
  bibauthor: Torsten Hothorn
  pages:
  - '640'
  - '641'
- slug: rmed2020
  author:
  - Elizabeth J. Atkinson
  - Peter D. Higgins
  - Denise Esserman
  - Michael J. Kane
  - Steven J. Schwager,
  title: 'R Medicine 2020: The Power of Going Virtual'
  bibtitle: 'R Medicine 2020: The Power of Going Virtual'
  bibauthor: |-
    Elizabeth J. Atkinson and Peter D. Higgins and Denise
              Esserman and Michael J. Kane and Steven J.
              Schwager,
  pages:
  - '642'
  - '647'
- slug: whyr2021
  author:
  - Mustafa Cavus
  - Olgun Aydin
  - Ozan Evkaya
  - Ozancan Ozdemir
  - Deniz Bezer
  - Ugur Dar
  title: Conference Report of Why R? Turkey 2021
  bibtitle: Conference Report of Why R? Turkey 2021
  bibauthor: |-
    Mustafa Cavus and Olgun Aydin and Ozan Evkaya and Ozancan
              Ozdemir and Deniz Bezer and Ugur Dar
  pages:
  - '648'
  - '653'
